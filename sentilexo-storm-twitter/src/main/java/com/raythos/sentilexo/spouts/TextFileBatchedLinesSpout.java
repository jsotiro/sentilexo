/*
 * Copyright 2014 (c) Raythos Interactive Ltd.  http://www.raythos.com
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.raythos.sentilexo.spouts;

import java.io.IOException;
import java.text.ParseException;
import java.util.Map;

import storm.trident.operation.TridentCollector;
import storm.trident.spout.IBatchSpout;
import backtype.storm.Config;
import backtype.storm.task.TopologyContext;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;
import com.raythos.sentilexo.twitter.TwitterQueryResultItemAvro;
import com.raythos.sentilexo.twitter.domain.TwitterQueryResultItemMapper;
import com.raythos.sentilexo.common.utils.AppProperties;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import twitter4j.Status;

/**
 * A Spout that emits lines from text files in batches.
 *
 * Kafka round trip
 *
 * @author yanni
 */
@SuppressWarnings({"serial", "rawtypes"})
public class TextFileBatchedLinesSpout implements IBatchSpout {

    protected static Logger log = LoggerFactory.getLogger(TextFileBatchedLinesSpout.class);
    protected JSONFileReaderWorker worker;

    public JSONFileReaderWorker getFileWorker() {
        return worker;
    }

    public TextFileBatchedLinesSpout() throws IOException {
        this(5);
    }

    public TextFileBatchedLinesSpout(int batchSize) throws IOException {
        this.worker = new JSONFileReaderWorker();
        this.worker.setBatchSize(batchSize);
        this.worker.setBufferSize(batchSize * 1000);
    }

    void scanPathForFileReads() {
        int filestoProcess = this.worker.scanForFilesFromPath();
        log.trace(filestoProcess + " JSON files scanned for processing");
        try {
            if (filestoProcess > 0) {
                this.worker.startReadingFiles();
            }
        } catch (IOException ex) {
            log.error("Error when starting reading files. The exception was ", ex);
        }
    }

    @Override
    public void open(Map conf, TopologyContext context) {
        // init
        this.worker.setQueryName(AppProperties.getProperty("QueryName"));
        this.worker.setQueryTerms(AppProperties.getProperty("QueryTerms"));
        log.trace("Stream basic properties loaded and configured from " + AppProperties.getPropertiesFile());
        scanPathForFileReads();
    }

    private Values getNextTweet() {
        Values result = null;
        if (worker.getBuffer().size() > 0) {
            String lineToProcess = (String) worker.getBuffer().get(0);
            Status status = worker.getStatusFromRawJsonLine(lineToProcess);
            if (status != null) {
                byte[] data = getSerialisedStatusObject(status);
                result = new Values(data);

            }
        }
        return result;
    }

    @Override
    public void emitBatch(long batchId, TridentCollector collector) {
        // emit batchSize 
        Values result = null;
        for (int i = 0; i < this.worker.getBuffer().size(); i++) {
            result = getNextTweet();
            worker.getBuffer().remove(0);
            if (result != null) {
                collector.emit(result);
            }
        }

        if (worker.isFinished()) {
            scanPathForFileReads();
        } else {
            try {
                worker.readNextBacthOfFileLines();
            } catch (IOException ex) {
                log.error("error reading next batch. The exception was ", ex);
            }
        }

    }

    @Override
    public void ack(long batchId) {
        // nothing to do here
    }

    @Override
    public void close() {
        // nothing to do here
    }

    @Override
    public Map getComponentConfiguration() {
        // no particular configuration here
        return new Config();
    }

    @Override
    public Fields getOutputFields() {
        return new Fields("bytes");
    }

    byte[] getSerialisedStatusObject(Status status) {
        byte[] data = null;
        log.trace("Posting Status with id " + status.getId() + " from File " + this.worker.getFilename());
        try {
            TwitterQueryResultItemAvro tqri = new TwitterQueryResultItemAvro();
            tqri = TwitterQueryResultItemMapper.mapItem(this.worker.getQueryName(), this.worker.getQueryTerms(), status);
            data = TwitterQueryResultItemMapper.getAvroSerialized(tqri);
            log.trace("AVRO serialised Status with id " + status.getId() + "Â was obtained from file" + this.worker.getFilename());
        } catch (Exception e) {
            log.error("Error when emmitting bytes. The exception was " + e);
        }
        return data;
    }

    public static void main(String[] args) throws IOException, ParseException {
        TextFileBatchedLinesSpout spout = new TextFileBatchedLinesSpout(5);
        String testBasePath = "/Users/yanni/sentidata";
        try {
            spout.getFileWorker().setBasePath(testBasePath);
            spout.open(null, null);
            while (!spout.getFileWorker().hasNoMoreData()) {
                spout.getNextTweet();
            }
            log.trace("Status items read : " + spout.getFileWorker().getStatusesRead());
            System.exit(0);
        } catch (Exception e) {

            log.error("exception with error: " + e.getMessage());
            System.exit(-10);
        }
    }

}
